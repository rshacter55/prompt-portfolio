{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4954173d-6a82-4ffb-a822-cac097eb1854",
   "metadata": {},
   "source": [
    "Explain the key differences between GPT, BERT, and T5 language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0c14-60ba-413c-b6fa-7f1278489bb0",
   "metadata": {},
   "source": [
    "# code taken from openai quickstart page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7af2cfc0-35db-4bed-9b9e-c97ab95f5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "   api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c7c3f-7fe9-4766-bd4a-7fd52a135299",
   "metadata": {},
   "source": [
    "# Chat Completions API - Poetic Assisstent - Explains\n",
    "Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation.\n",
    "\n",
    "An example Chat Completions API call looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbd445f0-1307-4853-85dc-37df27bda227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of coding, a pattern we find,\n",
      "Recursive functions, a clever design.\n",
      "A task splits in two, then tackled with grace,\n",
      "Until a base case, the loop we embrace.\n",
      "\n",
      "Like a mirror reflecting, time and again,\n",
      "Calling itself, a loop without end.\n",
      "A function divine, in its looping spree,\n",
      "Unraveling mysteries, unlocking the key.\n",
      "\n",
      "In the labyrinth of data, it journeys deep,\n",
      "A dance with logic, a promise to keep.\n",
      "Repeating, reshaping, with each recursive call,\n",
      "A tale of elegance, the loom of code's thrall.\n",
      "\n",
      "So heed the call of recursion's sweet rhyme,\n",
      "A true programming marvel, a rhythm so prime.\n",
      "In this loop of loops, a coding delight,\n",
      "A recursive voyage, through the day and the night.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af14eb9-014c-4c43-9ca3-56d122735ca9",
   "metadata": {},
   "source": [
    "# Chat Completions API - Q&A\n",
    "Chat models take a list of messages as input and return a model-generated message as output. Although the chat format is designed to make multi-turn conversations easy, it’s just as useful for single-turn tasks without any conversation.\n",
    "\n",
    "An example Chat Completions API call looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "edec62ba-edc3-4e48-8fe1-9391e47fd3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The World Series in 2020 was played at Globe Life Field in Arlington, Texas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5c1d6-182e-439c-b439-c165253c1855",
   "metadata": {},
   "source": [
    "# Chat Completion JSON mode\n",
    "A common way to use Chat Completions is to instruct the model to always return a JSON object that makes sense for your use case, by specifying this in the system message. While this does work in some cases, occasionally the models may generate output that does not parse to valid JSON objects.\n",
    "\n",
    "To prevent these errors and improve model performance, when using gpt-4o, gpt-4-turbo, or gpt-3.5-turbo, you can set response_format to { \"type\": \"json_object\" } to enable JSON mode. When JSON mode is enabled, the model is constrained to only generate strings that parse into valid JSON object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39b7a7b1-8948-46fb-b5be-7b3034ed07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"winner\": \"Los Angeles Dodgers\",\n",
      "    \"year\": 2020,\n",
      "    \"runner_up\": \"Tampa Bay Rays\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb2baa-ed50-411d-8004-3ad450d53921",
   "metadata": {},
   "source": [
    "# Chat - Reproducible outputs - Seed and Fignerprint\n",
    "Chat Completions are non-deterministic by default (which means model outputs may differ from request to request). That being said, we offer some control towards deterministic outputs by giving you access to the seed parameter and the system_fingerprint response field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972cf3f-6b37-484e-948b-5123a1b378ba",
   "metadata": {},
   "source": [
    "# The following Examples are from the Examples in the Open AI Site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c8a1f-4fd0-4623-a53d-b849a0292d93",
   "metadata": {},
   "source": [
    "# Prompt - Grammer Correction\n",
    "Convert ungrammatical statements into standard English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b935d639-7793-4541-9efc-69b7921bc1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She did not go to the market.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with statements, and your task is to convert them to standard English.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"She no went to the market.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdcc222-dd09-4ba2-9b6e-66f5fc668e7f",
   "metadata": {},
   "source": [
    "# Prompt - Parse unstructured data\n",
    "Create tables from unstructured text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43deab7e-1c25-4189-b7da-90727197269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit,Color,Flavor\n",
      "neoskizzles,purple,candy\n",
      "loheckles,grayish blue,tart\n",
      "pounits,bright green,savory\n",
      "loopnovas,neon pink,cotton candy\n",
      "glowls,pale orange,sour, bitter\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2789c-6d08-46ba-9de3-2fb6870f6e8d",
   "metadata": {},
   "source": [
    "# Prompt - Calculate Time Complexity\n",
    "Find the time complexity of a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27673396-8bb1-432b-ad4c-83e92af8d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time complexity of this code is O(n*k) because there are two nested loops where the outer loop runs n times and the inner loop runs k times. The variable accum is incremented by i*k times, where i ranges from 0 to n-1.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with Python code, and your task is to calculate its time complexity.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"def foo(n, k):\\n        accum = 0\\n        for i in range(n):\\n            for l in range(k):\\n                accum += i\\n        return accum\"\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39526396-4f37-4918-bf03-47fd3a1a3038",
   "metadata": {},
   "source": [
    "# Prompt - Keywords\n",
    "Extract keywords from a block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb9d9e71-dd3b-4e23-8c7a-54cd2a0d9caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Black-on-black ware\n",
      "- Puebloan Native American\n",
      "- Pottery tradition\n",
      "- Northern New Mexico\n",
      "- Reduction-fired blackware\n",
      "- Pueblo artists\n",
      "- Smooth surface\n",
      "- Selective burnishing\n",
      "- Refractory slip\n",
      "- Carving\n",
      "- Incising designs\n",
      "- Polishing\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be provided with a block of text, and your task is to extract a list of keywords from it.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhóge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=64,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccebf8-a431-4baf-a0d9-a1a230c2577c",
   "metadata": {},
   "source": [
    "# Prompt - Python bug fixer\n",
    "Find and fix bugs in source code.\n",
    "My note: Needs more tokens to get bug and corrected code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82ef4b47-de3f-4ca8-b479-074ba796a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the problems with your code:\n",
      "\n",
      "1. The import statement is wrong. It should be \"import random\" not \"import Random\".\n",
      "2. You are trying to concatenate string with integers in the line where you defined \"question\". You need to convert integers to string using str() function.\n",
      "3. There is a syntax error in your if statement. You used one equal sign (=) instead of two (==) for comparison.\n",
      "4. \"Well done!\" needs to be in quotes because it's a string.\n",
      "5. The variables a and b need to be inside the loop if you want to generate a new pair of random numbers each time.\n",
      "\n",
      "Here is the corrected code:\n",
      "\n",
      "```python\n",
      "import random\n",
      "for i in range(10):\n",
      "    a = random.randint(1,12)\n",
      "    b = random.randint(1,12)\n",
      "    question = \"What is \"+ str(a) +\" x \"+ str(b) +\"? \"\n",
      "    answer = input(question)\n",
      "    if int(answer) == a*b:\n",
      "        print(\"Well done!\")\n",
      "    else:\n",
      "        print(\"No.\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You will be provided with a piece of Python code, and your task is to find and fix bugs in it.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\"import Random\\n    a = random.randint(1,12)\\n    b = random.randint(1,12)\\n    for i in range(10):\\n        question = \\\"What is \\\"+a+\\\" x \\\"+b+\\\"? \\\"\\n        answer = input(question)\\n        if answer = a*b\\n            print (Well done!)\\n        else:\\n            print(\\\"No.\\\")\"\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 250,\n",
    "    top_p = 1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4396b-67a6-4198-afc9-c3eaa9a2aa04",
   "metadata": {},
   "source": [
    "# Prompt - Tweet classifier\n",
    "Detect sentiment in a tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90e4dbfc-b47d-46d7-8e3d-ecf7de155728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I thought the new Batman movie was a cliche\"\n",
    "      }  \n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=64,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6623e5-8ea6-4754-88f7-bb7266cc2db5",
   "metadata": {},
   "source": [
    "# Prompt - Mood to color\n",
    "Turn a text description into a color.\n",
    "My note: not so sure about the results of this one : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3789154-336f-49cb-9c01-50cc1d4d4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"css_code\": \"background-color: #4f5158;\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages= [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will be provided with a description of a mood, and your task is to generate the CSS code for a color that matches it. Write your output in json with a single key called \\\"css_code\\\".\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"sunset over the dark city scape\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=64,\n",
    "    top_p=1   \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70355d5-aef8-4c43-bda7-71c8f91d8b22",
   "metadata": {},
   "source": [
    "# Prompt - Interview questions for an innovative product manager\n",
    "Create interview questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ecd5675-dde6-49c3-ab6e-4c1a4082c25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can you tell us about a recent product innovation you spearheaded and how it has impacted your company?\n",
      "2. How do you stay current with industry trends and emerging technologies to drive innovation in your products?\n",
      "3. What strategies do you use to gather customer feedback and incorporate it into product development?\n",
      "4. How do you prioritize features and enhancements for your products to ensure they meet customer needs and stay ahead of competitors?\n",
      "5. Can you walk us through your process for testing and iterating on new product ideas before launch?\n",
      "6. How do you collaborate with cross-functional teams, such as engineering and marketing, to bring innovative products to market?\n",
      "7. How do you measure the success of your product innovations, and what metrics do you use to track their performance?\n",
      "8. Can you share a specific challenge you faced as a product manager and how you overcame it to successfully launch a new product?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Create a list of 8 questions for an interview with an innovative product manager.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=200,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6820f-a003-45bc-b4f5-3fc727dc5f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09495054-446e-4d7a-8b6a-1840bda726e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f9629-a162-433d-96f3-fdc3fa6903ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0a894-ec34-4855-88f3-0d171f82d69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
